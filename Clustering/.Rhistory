student.data <- data.frame(
student_id = c (1:5),
emp_name = c("Raj","Khush","Manpreet","Armaan","Garry"),
marks = c(623.3,515.2,611.0,729.0,843.25))
# output
print(student.data)
# The structure of the data frame can be seen by using str() function
str(student.data)
# The statistical summary and nature of the data can be obtained
# by applying summary() function.
summary(student.data)
#reading titanic dataset..
df <- Titanic
df.info()
str(df)
sum(is.na(df))
sum(is.na(df$Class))
df <- read.csv("*C:/Users/king/Document/test.csv")
df <- read.csv('*C:/Users/king/Documents/test.csv')
#reading titanic dataset..
df <- read.csv('C:/Users/king/Documents/test.csv')
# counting na in df
sum(is.na(df))
# delete all the empty rows
del <- na.omit(df)
sum(is.na(del))
summary(df)
str(df)
# mean imputation
# mean imputation is suitable for continuous variable
library(Hmisc)
sum(is.na(df))
sum(is.na(df$Fare))
h_mean <- impute(df$Fare, mean())
h_mean <- impute(df$Fare, mean(df$Fare))
# logical approach
df$Fare(is.na(df$Fare)) <- mean(df$Fare, na.rm = TRUE)
h_median <- impute(df$Fare, median(df$Fare))
sum(is.na(df$Fare))
sum(is.na(h_mean))
h_mean.head()
h_mean
is.na(h_mean)
sum(is.na(h_mean))
h_mean <- impute(df$Fare, mean(df$Fare))
h_median <- impute(df$Fare, median(df$Fare))
sum(is.na(h_mean))
h_mean <- impute(df$Fare, mean)
sum(is.na(h_mean))
h_median <- impute(df$Fare, median)
sum(is.na(h_median))
#reading titanic dataset..
df <- read.csv('C:/Users/king/Documents/test.csv')
# missing value
# Missing values in data is a common phenomenon in real world problems
# handle missing values effectively is a required step to reduce bias
# and to produce powerful models
# counting na in df
sum(is.na(df))
# delete all the empty rows
del <- na.omit(df)
sum(is.na(del))
# mean imputation and median inputation
# mean imputation is suitable for continuous variable and data is
# normaly distribution
# median is appropriate for continuous as well as discrete variabless
# even of the data is skewed because of outliers
sum(is.na(df$Fare))
# using library
library(Hmisc)
h_mean <- impute(df$Fare, mean)
sum(is.na(h_mean))
h_median <- impute(df$Fare, median)
sum(is.na(h_median))
#2 way anova
anova_2 <- aov(time ~ poison + treat, data = df)
library(dplyr)
PATH <- "https://raw.githubusercontent.com/guru99-edu/R-Programming/master/poisons.csv"
df <- read.csv(PATH) %>%
select(-X) %>%
mutate(poison = factor(poison, ordered = TRUE))
head(df, 5)
# ABOUT DATA:
# Time: Survival time of the animal
# poison: Type of poison used: factor level: 1,2 and 3
# treat: Type of treatment used: factor level: 1,2 and 3
# ONE WAY ANOVA
# HYPOTHESIS
# h0 = no difference in survival time average between group
# h1 = survival time average is different for at least one group.
#print summary statistics
df %>%
group_by(poison) %>%
summarise(count = n(),
mean_time = mean(time, na.rm = TRUE),
sd_time = sd(time, na.rm = TRUE))
#one-way anova table
anova_ = aov(time ~ poison, data = df)
summary(anova_)
#tukey HSD test
#pairwise comparison
TukeyHSD(anova_)
#2 way anova
anova_2 <- aov(time ~ poison + treat, data = df)
summary(anova_2)
mpg
library(ggplot2)
ggplot::mpg
ggplot2::mpg
data <- ggplot2::mpg
hist(data$hwy,
xlab = "hwy",
main = "Histogram of hwy")
boxplot(data$hwy)
boxplot(outlier_ind)
lower_bound <- quantile(data$hwy, 0.025)
upper_bound <- quantile(data$hwy, 0.975)
outlier_ind <- which(data$hwy < lower_bound | dat$hwy > upper_bound)
boxplot(outlier_ind)
lower_bound <- quantile(data$hwy, 0.025)
upper_bound <- quantile(data$hwy, 0.975)
data$hwy <- which(data$hwy < lower_bound | data$hwy > upper_bound)
boxplot(data$hwy
lower_bound <- quantile(data$hwy, 0.025)
upper_bound <- quantile(data$hwy, 0.975)
otl <- which(data$hwy < lower_bound | data$hwy > upper_bound)
#removing outliers
lower_bound <- quantile(data$hwy, 0.025)
upper_bound <- quantile(data$hwy, 0.975)
otl <- which(data$hwy < lower_bound | data$hwy > upper_bound)
boxplot(otl)
data$hwy <- which(data$hwy < lower_bound | data$hwy > upper_bound)
lower_bound <- quantile(data$hwy, 0.025)
upper_bound <- quantile(data$hwy, 0.975)
data$hwy <- which(data$hwy < lower_bound | data$hwy > upper_bound)
hist(otl,
hist(otl,
xlab = "hwy",
main = "Histogram of hwy
hist(otl,
xlab = "hwy",
main = "Histogram of hwy")
data <- ggplot2::mpg
#hostogram
hist(data$hwy,
xlab = "hwy",
main = "Histogram of hwy")
#boxplot
boxplot(data$hwy)
#removing outliers
lower_bound <- quantile(data$hwy, 0.025)
upper_bound <- quantile(data$hwy, 0.975)
otl <- which(data$hwy < lower_bound | data$hwy > upper_bound)
#after removing outliers
boxplot(otl)
hist(otl,
xlab = "hwy",
main = "Histogram of hwy")
#boxplot
boxplot(data$hwy)
#after removing outliers
boxplot(otl)
hist(otl,
xlab = "hwy",
main = "Histogram of hwy")
df1 = data.frame(CustomerId = c(1:6),
Product = c("Oven","Television","Mobile",
"WashingMachine","Lightings","Ipad"))
df1
# data frame 2
df2 = data.frame(CustomerId = c(2, 4, 6, 7, 8),
State = c("California","Newyork","Santiago",
"Texas","Indiana"))
df2
df= df1 %>% inner_join(df2,by="CustomerId")
df
library(dplyr)
d= df1 %>% inner_join(df2,by="CustomerId")
d
df= df1 %>% full_join(df2,by="CustomerId")
df
#inner join
#Inner Join in R is the simplest and most common type of join.
#It is also known as simple join or Natural Join. Inner join
#returns the rows when matching condition is met.
d= df1 %>% inner_join(df2,by="CustomerId")
d
#outer join
#Outer Join in  R combines the results of both left and right outer joins.
#The joined table will contain all records from both the tables
df= df1 %>% full_join(df2,by="CustomerId")
df
#The LEFT JOIN in R returns all records from the left dataframe,
#and the matched records from the right dataframe
df= df1 %>% left_join(df2,by="CustomerId")
df
#The RIGHT JOIN in R returns all records from the right dataframe,
#and the matched records from the left dataframe
df= df1 %>% right_join(df2,by="CustomerId")
df
# FIND THE NUMBERS OF OPTIMUM CLUSTERS
library(cluster)
#reading the in-built iris dataset
df <- iris
head(df)
#reading the first four columns of the dataset
df <- df[,1:4]
#dimension of the dataset
dim(df)
#structure of dataframe
str(df)
# FIND THE NUMBERS OF OPTIMUM CLUSTERS
library(cluster)
fviz_nbclust(df, kmeans, method = 'wss') +
geom_vline(xintercept = 3, linetype = 5, col = 'orange')
library(factoextra)
fviz_nbclust(df, kmeans, method = 'wss') +
geom_vline(xintercept = 3, linetype = 5, col = 'orange')
km.res= kmeans(df, 3, nstart = 40)
#USEFULL INFORMATION ABOUT CLUSTERS
km.res
km.res$totss
km.res$betweenss
139.5968/196
aggregate(df, by = list(cluster = km.res$cluster), mean)
df_m = cbind(df, cluster = km.res$cluster)
head(df_m)
View(df_m)
fviz_cluster(km.res, data = df,
palette = c('#2E9FDF', '#00AFBB', '#E7B800'),
ellipse.type = 'euclid',
star.plot = TRUE,
repel = TRUE,
ggtheme = theme())
res.dist = dist(df, method = 'euclidean')
head(res.dist)
round(as.matrix(res.dist)[1:3, 1:3], 1)
fviz_dist(res.dist)
res.hc= hclust(d = res.dist, method = 'ward.D2')
fviz_dend(res.hc, cex = 0.5)
fviz_dend(res.hc, k = 4,
cex = 0.5,
k_colors = c('#2E9FDF', '#00AFBB', '#E7B800'),
color_labels_by_k = TRUE,
rect = TRUE)
# MORE BETTER DIAGRAMS
# CUT TREE
grp= cutree(res.hc, k = 3)
head(grp, n=3)
table(grp)
rownames(df)[grp ==1]
fviz_cluster(list(data = df, cluster = grp),
palette = c('#2E9FDF', '#00AFBB', '#E7B800', '#FC4E07'),
ellipse = TRUE,
ellipse.type = "convex",
repel = TRUE,
show.clust.cent = FALSE,
ggtheme = theme_classic())
#reading the in-built iris dataset
df <- iris
head(df)
#reading the first four columns of the dataset
df <- df[,1:4]
#dimension of the dataset
dim(df)
#structure of dataframe
str(df)
# FIND THE NUMBERS OF OPTIMUM CLUSTERS
library(cluster)
library(factoextra)
fviz_nbclust(df, kmeans, method = 'wss') +
geom_vline(xintercept = 3, linetype = 5, col = 'orange')
# K MEANS CLUSTERING
set.seed(123)
km.res= kmeans(df, 3, nstart = 40)
#USEFULL INFORMATION ABOUT CLUSTERS
km.res
km.res$totss
km.res$betweenss
fviz_cluster(list(data = df, cluster = grp),
palette = c('#2E9FDF', '#00AFBB', '#E7B800', '#FC4E07'),
ellipse = TRUE,
ellipse.type = "convex",
repel = TRUE,
show.clust.cent = FALSE,
ggtheme = theme_classic())
fviz_cluster(list(data = df, cluster = grp),
palette = c('#2E9FDF', '#E7B800', '#FC4E07'),
ellipse = TRUE,
ellipse.type = "convex",
repel = TRUE,
show.clust.cent = FALSE,
ggtheme = theme_classic())
fviz_cluster(list(data = df, cluster = grp),
palette = c('#2E9FDF', '#E7B800', '#FC4E07'),
ellipse = TRUE,
ellipse.type = "convex",
show.clust.cent = FALSE,
ggtheme = theme_classic())
fviz_cluster(list(data = df, cluster = grp),
palette = c('#2E9FDF', '#E7B800', '#FC4E07'),
ellipse = FALSE,
ellipse.type = "convex",
show.clust.cent = TRUE,
ggtheme = theme_classic())
fviz_cluster(list(data = df, cluster = grp),
palette = c('#2E9FDF', '#E7B800', '#FC4E07'),
ellipse = TRUE,
ellipse.type = "convex",
show.clust.cent = TRUE,
ggtheme = theme_classic())
?fviz_cluster
fviz_cluster(list(data = df, cluster = grp),
palette = c('#2E9FDF', '#E7B800', '#FC4E07'),
stand = FALSE
ellipse = TRUE,
ellipse.type = "convex",
show.clust.cent = TRUE,
ggtheme = theme_classic())
fviz_cluster(list(data = df, cluster = grp),
palette = c('#2E9FDF', '#E7B800', '#FC4E07'),
stand = FALSE
ellipse = TRUE,
ellipse.type = "convex",
show.clust.cent = TRUE,
ggtheme = theme_classic())
fviz_cluster(list(data = df, cluster = grp),
palette = c('#2E9FDF', '#E7B800', '#FC4E07'),
stand = FALSE,
ellipse = TRUE,
ellipse.type = "convex",
show.clust.cent = TRUE,
ggtheme = theme_classic())
?fviz_cluster
fviz_cluster(list(data = df, cluster = grp),
palette = c('#2E9FDF', '#E7B800', '#FC4E07'),
ellipse = TRUE,
ellipse.type = "convex",
show.clust.cent = FALSE,
ggtheme = theme_classic())
# PROFILE OF EACH CLISTER
# SEE MEANS OF VARIABLES IN ORIGINAL DATA, CLUSTER WISE
aggregate(df, by = list(cluster = km.res$cluster), mean)
fviz_dend(res.hc, k = 3,
cex = 0.5,
k_colors = c('#2E9FDF', '#00AFBB', '#E7B800'),
color_labels_by_k = TRUE,
rect = TRUE)
res.hc= hclust(d = res.dist, method = 'ward.D2')
fviz_dend(res.hc, cex = 0.5)
fviz_dend(res.hc, k = 3,
cex = 0.5,
k_colors = c('#2E9FDF', '#00AFBB', '#E7B800'),
color_labels_by_k = TRUE,
rect = TRUE)
fviz_dend(res.hc, k = 3,
cex = 0.5,
k_colors = c('#2E9FDF', '#FC4E07', '#00AFBB'),
color_labels_by_k = TRUE,
rect = TRUE)
fviz_dend(res.hc, k = 3,
cex = 0.5,
k_colors = c('#2E9FDF', '#FC4E07', , '#E7B800'),
color_labels_by_k = TRUE,
rect = TRUE)
fviz_dend(res.hc, k = 3,
cex = 0.5,
k_colors = c('#2E9FDF', '#FC4E07', '#E7B800'),
color_labels_by_k = TRUE,
rect = TRUE)
df <- iris
head(df)
#HIERARCHICAL CLUSTERING
res.hc= hclust(d = res.dist, method = 'ward.D2')
fviz_dend(res.hc, cex = 0.5)
fviz_dend(res.hc, k = 3,
cex = 0.5,
k_colors = c('#2E9FDF', '#FC4E07', '#E7B800'),
color_labels_by_k = TRUE,
rect = TRUE)
# FIND THE NUMBERS OF OPTIMUM CLUSTERS
library(cluster)
library(factoextra)
fviz_nbclust(df, kmeans, method = 'wss') +
geom_vline(xintercept = 3, linetype = 5, col = 'orange')
#reading the in-built iris dataset
df <- iris
head(df)
#reading the first four columns of the dataset
df <- df[,1:4]
#dimension of the dataset
dim(df)
#structure of dataframe
str(df)
# FIND THE NUMBERS OF OPTIMUM CLUSTERS
library(cluster)
library(factoextra)
fviz_nbclust(df, kmeans, method = 'wss') +
geom_vline(xintercept = 3, linetype = 5, col = 'orange')
# K MEANS CLUSTERING
set.seed(123)
km.res= kmeans(df, 3, nstart = 40)
#USEFULL INFORMATION ABOUT CLUSTERS
km.res
km.res$totss
km.res$betweenss
fviz_cluster(list(data = df, cluster = grp),
palette = c('#2E9FDF', '#E7B800', '#FC4E07'),
ellipse = TRUE,
ellipse.type = "convex",
show.clust.cent = FALSE,
ggtheme = theme_classic())
# PROFILE OF EACH CLISTER
# SEE MEANS OF VARIABLES IN ORIGINAL DATA, CLUSTER WISE
aggregate(df, by = list(cluster = km.res$cluster), mean)
res.hc= hclust(d = res.dist, method = 'ward.D2')
fviz_dend(res.hc, cex = 0.5)
fviz_dend(res.hc, k = 3,
cex = 0.5,
k_colors = c('#2E9FDF', '#FC4E07', '#E7B800'),
color_labels_by_k = TRUE,
rect = TRUE)
fviz_cluster(list(data = df, cluster = grp),
palette = c('#2E9FDF', '#E7B800', '#FC4E07'),
ellipse = TRUE,
ellipse.type = "convex",
show.clust.cent = FALSE,
ggtheme = theme_classic())
#reading the in-built iris dataset
df <- iris
head(df)
#reading the first four columns of the dataset
df <- df[,1:4]
#dimension of the dataset
dim(df)
#structure of dataframe
str(df)
# FIND THE NUMBERS OF OPTIMUM CLUSTERS
library(cluster)
library(factoextra)
fviz_nbclust(df, kmeans, method = 'wss') +
geom_vline(xintercept = 3, linetype = 5, col = 'orange')
# K MEANS CLUSTERING
set.seed(123)
km.res= kmeans(df, 3, nstart = 40)
#USEFULL INFORMATION ABOUT CLUSTERS
km.res
km.res$totss
km.res$betweenss
fviz_cluster(list(data = df, cluster = grp),
palette = c('#2E9FDF', '#E7B800', '#FC4E07'),
ellipse = TRUE,
ellipse.type = "convex",
show.clust.cent = FALSE,
ggtheme = theme_classic())
fviz_cluster(km.res, data = df),
palette = c('#2E9FDF', '#E7B800', '#FC4E07'),
ellipse = TRUE,
ellipse.type = "convex",
show.clust.cent = FALSE,
ggtheme = theme_classic())
fviz_cluster(km.res, data = df,
palette = c('#2E9FDF', '#E7B800', '#FC4E07'),
ellipse = TRUE,
ellipse.type = "convex",
show.clust.cent = FALSE,
ggtheme = theme_classic())
# PROFILE OF EACH CLISTER
# SEE MEANS OF VARIABLES IN ORIGINAL DATA, CLUSTER WISE
aggregate(df, by = list(cluster = km.res$cluster), mean)
res.hc= hclust(d = res.dist, method = 'ward.D2')
#HIERARCHICAL CLUSTERING
res.dist = dist(df, method = 'euclidean')
res.hc= hclust(d = res.dist, method = 'ward.D2')
fviz_dend(res.hc, cex = 0.5)
setwd("~/")
setwd("D:/self study/project/Grip_Internship/Clustering")
#reading the in-built iris dataset
df <- iris
head(df)
#reading the first four columns of the dataset
df <- df[,1:4]
#dimension of the dataset
dim(df)
#structure of dataframe
str(df)
# FIND THE NUMBERS OF OPTIMUM CLUSTERS
library(cluster)
library(factoextra)
fviz_nbclust(df, kmeans, method = 'wss') +
geom_vline(xintercept = 3, linetype = 5, col = 'orange')
km.res= kmeans(df, 3, nstart = 40)
#USEFULL INFORMATION ABOUT CLUSTERS
km.res
fviz_cluster(km.res, data = df,
palette = c('#2E9FDF', '#E7B800', '#FC4E07'),
ellipse = TRUE,
ellipse.type = "convex",
show.clust.cent = FALSE,
ggtheme = theme_classic())
# PROFILE OF EACH CLISTER
# SEE MEANS OF VARIABLES IN ORIGINAL DATA, CLUSTER WISE
aggregate(df, by = list(cluster = km.res$cluster), mean)
#HIERARCHICAL CLUSTERING
res.dist = dist(df, method = 'euclidean')
res.hc= hclust(d = res.dist, method = 'ward.D2')
fviz_dend(res.hc, cex = 0.5)
fviz_dend(res.hc, k = 3,
cex = 0.5,
k_colors = c('#2E9FDF', '#FC4E07', '#E7B800'),
color_labels_by_k = TRUE,
rect = TRUE)
